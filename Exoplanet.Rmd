---
title: "Exoplanet"
output:
  html_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

### KOI_table

Dataset from the NASA : source :
<https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=cumulative>

```{r}
KOI_table_full = read.csv("KOI_table_2025.03.23_01.50.59.csv",comment.char = "#",header = TRUE, stringsAsFactors = FALSE)
```

### Librairies

```{r}
library(gitcreds) # git
library(dplyr) # selection
library(GGally) # pair plots
library(corrplot) # correlation heatmap
library(ggplot2) # box plots
library(rgl) # 3D plot
```

### Data exploration

First of all, we begin by looking into our dataset (features, dimension, type)

```{r}
head(KOI_table_full)
```

```{r}
names(KOI_table_full)
```

```{r}
str(KOI_table_full)
```

```{r}
summary(KOI_table)
```

```{r}
dim(KOI_table_full)
class(KOI_table_full)
```

We have to remove the unnecessary, unrelevant columns which are : 
(data that are not relevant from a scientific, astronomic point of view)
- object id 
- object name 
- satellite name 
- pdisposition (Kepler) (less information than disposition) 
- koi_depth_err1: Transit Depth Upper Unc. [ppm] (no information : NA) 
- koi_depth_err2: Transit Depth Lower Unc. [ppm] (no information : NA) 
- koi_tce_delivname: TCE Delivery (it's just the delivery type)

We also don't take into account first the upper and lower bound of some features
We will include them in our analysis later

We choose to work on the class (confirmed, false positive) instead of
the score, so we remove it

We also remove the still unconfirmed candidate exoplanets because we
only work with the ones whose analyse is finished

```{r}
column_rm = c("kepid","kepoi_name","kepler_name","koi_pdisposition",
              "koi_score",
              "koi_teq_err1","koi_teq_err2","koi_tce_delivname")
KOI_table <- KOI_table_full %>% select(-all_of(column_rm))
KOI_table <- KOI_table_full %>% filter(koi_disposition != "CANDIDATE")


real_columns_to_remove <- c("kepid", "kepoi_name", "kepler_name", "koi_pdisposition", "koi_score", 
                       "koi_fpflag_nt", "koi_fpflag_ss", "koi_fpflag_co", "koi_fpflag_ec", 
                       "ra", "dec", "koi_kepmag", "koi_model_snr", "koi_tce_plnt_num", 
                       "koi_tce_delivname")

c("koi_period_err1", "koi_period_err2", "koi_time0bk_err1", "koi_time0bk_err2", 
                   "koi_impact_err1", "koi_impact_err2", "koi_duration_err1", "koi_duration_err2", 
                   "koi_depth_err1", "koi_depth_err2", "koi_prad_err1", "koi_prad_err2", 
                   "koi_insol_err1", "koi_insol_err2", "koi_steff_err1", "koi_steff_err2", 
                   "koi_slogg_err1", "koi_slogg_err2", "koi_srad_err1", "koi_srad_err2", "koi_teq_err1", "koi_teq_err2") 

```

We now have :

```{r}
head(KOI_table)
dim(KOI_table)
names(KOI_table)
```

We encode binarly the class : 0 if false positive, 1 if confirmed
exoplanet

```{r}
KOI_table$koi_disposition <- ifelse(KOI_table$koi_disposition == "CONFIRMED", 1, 0)
unique(KOI_table$koi_disposition)
```

### Data cleaning

#### NA values

Let's work on missing datapoint

```{r}
colSums(is.na(KOI_table))
```

We have different options : - remove NA - fill the NA with specific
value : mean, mice, etc...

If we remove NA, we have : We lose 653 observations in total

```{r}
KOI_table = na.omit(KOI_table)
dim(KOI_table)
dim(KOI_table)
```

### Data analyse

```{r}
attach(KOI_table)
```

#### Class distribution

We can look for class imbalance : Class imbalance can biased the model
towards the majority class - it can lead to poor recall or precision on
the minority class - it can draw boundaries too close to the minority
class (Log Reg, SVM)

```{r}
# Create a frequency table
table_koi <- table(KOI_table[["koi_disposition"]])
table_koi
# Convert to proportions
prop_koi <- prop.table(table_koi)
# Create a bar plot
barplot(prop_koi,
        col = rainbow(length(prop_koi)),  # Assign colors dynamically
        ylim = c(0,1),
        main = "Class Distribution",
        ylab = "Proportion",
        xlab = "KOI Disposition")
```

In our case, we can see imbalance between classes that we can handle
with for example : - Oversampling (SMOTE) or Undersampling

A revoir !

```{r small_plot, fig.width=5, fig.height=3}
# Per class
#for (feature in KOI_table){
#  boxplot(feature ~ koi_disposition, data = KOI_table, main = "Boxplot by Class")
#}
#dim(koi_disposition)
```

A revoir !

```{r}
### Pairplot for some features
#pairs(KOI_table)
#selected_features <- KOI_table[, c("koi_fpflag_nt", "koi_period", "koi_duration", "koi_teq", #"koi_steff", "koi_disposition")]
#ggpairs(selected_features, aes(color = "koi_disposition"))
#pairs(selected_features, aes(color="koi_disposition"))
```

#### Features distributions





##### Normality test

```{r}
n <- dim(KOI_table)
p <- dim(KOI_table)
```



##### Standardized data

Boxplots

```{r}
KOI_table_scaled <- KOI_table
KOI_table_scaled[, -which(names(KOI_table_scaled) == "koi_disposition")] <- scale(KOI_table_scaled[, -which(names(KOI_table_scaled) == "koi_disposition")])
boxplot(KOI_table_scaled, las = 2, main = "Standardized Features Boxplot")

KOI_table_scaled
```

#### Correlation

Correlation between features

```{r}
cor_matrix <- cor(KOI_table[, -which(names(KOI_table) == "koi_disposition")], use = "complete.obs")
corrplot(cor_matrix, method = "color", tl.cex = 0.6, tl.srt = 45)
```

Correlation between features and the class :

```{r}

```

#### Variance explained

##### PCA

We have a lot of features, PCA will help to reduce the dimension and
interpret the features

```{r}
pca <- prcomp(KOI_table[, -which(names(KOI_table) == "koi_disposition")], scale. = TRUE)
summary(pca)
```

```{r}
# Cumulative variance
var_explained <- pca$sdev^2 / sum(pca$sdev^2)
cumvar <- cumsum(var_explained)
plot(cumvar, 
     xlab = "Number of Principal Components", 
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b", pch = 19, col = "blue",
     main = "Cumulative Variance Explained by PCs")
abline(h = 0.9, col = "red", lty = 2)  # optional 90% guide
```

```{r}
# PCA loadings histogram
loadings <- as.data.frame(pca$rotation)  # Loadings matrix
loadings$Feature <- rownames(loadings)   # Feature names
loadings_long <- loadings %>% pivot_longer(cols = starts_with("PC"), names_to = "Principal_Component", values_to = "Loading")

ggplot(loadings_long %>% filter(Principal_Component == "PC1"), aes(x = reorder(Feature, Loading), y = Loading)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +  # Flip axes for readability
  theme_minimal() +
  labs(title = "PCA Loadings for PC1", x = "Features", y = "Loading Value")
```

First 2 principal components :

```{r}
# Select the first 2 principal components
df_pca <- data.frame(pca$x[, 1:2], label = koi_disposition)
df_pca
ggplot(df_pca, aes(x = PC1, y = PC2, color = label)) +
  geom_point() +
  theme_minimal()
```

First 3 principal components :

```{r}
# 3D Scatter Plot
df_pca <- data.frame(pca$x[, 1:3], label = koi_disposition)
plot3d(df_pca$PC1, df_pca$PC2, df_pca$PC3, 
       col = rainbow(length(unique(df_pca$label)))[as.factor(df_pca$label)],
       size = 5, 
       xlab = "PC1", ylab = "PC2", zlab = "PC3", 
       main = "3D PCA Visualization")
```

First 5 principal components :

```{r}
# Select the first 5 principal components
df_pca_5 <- data.frame(pca$x[, 1:5], label = koi_disposition)
# Parallel coordinate plot
ggparcoord(df_pca_5, columns = 1:5, groupColumn = "label", alphaLines = 0.5)
```

Entre parenthèse KOI_table_PCA = pca$x
On ne garde que les p premières composantes :
p = 20
Pour ..
cumsum(eigenvalues)[20]/tail(cumsum(eigenvalues),1)
% de la variance totale
KOI_table_PCA = pca$x[,1:20] dim(KOI_table_PCA)

#### Outliers

```{r}
library(mice)        # Handling missing data
library(VIM)         # Visualizing missing data
```

A revoir !

```{r}
outlier_indices_list <- list()  # Initialize empty list to store indices

for (feature_name in names(KOI_table)) {
  feature <- KOI_table[[feature_name]]  # Extract column
  
  if (is.numeric(feature)) {
    # Get boxplot stats
    stats <- boxplot.stats(feature)
    
    # Identify indices of outliers
    outlier_indices <- which(feature %in% stats$out)
    
    # Store in list with feature name as key
    outlier_indices_list[[feature_name]] <- outlier_indices
    
    # Plot boxplot with labels
    boxplot(feature,
            main = paste("Boxplot of", feature_name),
            xlab = feature_name)
  }
}

```

```{r}
# Filter out empty entries (features with no outliers)
non_empty_lists <- Filter(length, outlier_indices_list)
# Get the union of all outlier indices
all_outliers <- unique(unlist(non_empty_lists))
all_outliers
dim(all_outliers)
# We lose too much information by removing the outliers
# We can do transformations 
```

```{r}
# We can't use log or box-cox transformation because we also have negative values
# However, we can use Yeo-Johnson transformation
library(e1071)
not_skewness_features <- list()
for (feature in KOI_table) {
  hist(feature)
}

#for 
#skewness(df$feature1)
```

### Classification

```{r}
library(caret) # split data
```

We divide the dataset into a : - training set - a testing set

```{r}
set.seed(123)  # For reproducibility
train_index <- createDataPartition(koi_disposition, p = 0.8, list = FALSE)  # 80% training

train_set <- KOI_table[train_index, ]
val_set <- KOI_table[-train_index, ]
```

```{r}
head(train_set)
dim(train_set)
dim(val_set)
```

#### LDA

#### KNN

```{r}
library(class) # KNN
```

Let's try K-Nearest Neighbor method :

```{r}
#KOI_table.knn <- knn(train = , cl = koi_disposition, k = 3, prob = T)
```

#### SVM

#### Decision Tree


#### Logistic regression

Fitting the model on the dataset

```{r}
glm.fit <- glm(koi_disposition~., data = train_set, family=binomial)
summary(glm.fit)
```

Prediction on the testing set

```{r}
# Prediction on validation set
val_probs <- predict(glm.fit, newdata = val_set, type = "response")
# Class prediction
val_pred <- ifelse(val_probs > 0.5, 1, 0)
# Confusion matrix
table(Predicted = val_pred, Actual = val_set$koi_disposition)
# Accuracy
mean(val_pred == val_set$koi_disposition)
```

#### Comparison

### Incertitudes influence

```{r}

```
