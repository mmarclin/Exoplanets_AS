---
title: "Exoplanet"
output:
  html_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

### KOI_table

Dataset from the NASA : source :
<https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=cumulative>

```{r}
KOI_table_full = read.csv("KOI_table_2025.03.23_01.50.59.csv",comment.char = "#",header = TRUE, stringsAsFactors = FALSE)
```

### Librairies

```{r message=FALSE, warning=FALSE}
library(gitcreds) # git
library(dplyr) # selection
library(GGally) # pair plots
library(corrplot) # correlation heatmap
library(ggplot2) # box plots
library(rgl) # 3D plot
```

### Data exploration

#### Definitions

KOI: Kepler Object of Interest
TCE: Threshold-Crossing Event


#### Exploration

First of all, we begin by looking into our dataset (features, dimension, type)

```{r}
head(KOI_table_full)
```

```{r}
names(KOI_table_full)
```

```{r}
str(KOI_table_full)
```

```{r}
summary(KOI_table_full)
```

```{r}
dim(KOI_table_full)
class(KOI_table_full)
```
#### Input space

We have to remove the unnecessary, unrelevant columns which are : 
(data that are not relevant from a scientific, astronomic point of view)
- object id 
- object name 
- satellite name 
- pdisposition (Kepler) (less information than disposition) 
- koi_depth_err1: Transit Depth Upper Unc. [ppm] (no information : NA) 
- koi_depth_err2: Transit Depth Lower Unc. [ppm] (no information : NA) 
- koi_tce_delivname: TCE Delivery (it's just the delivery type) 
- koi_fpflag_nt: Not Transit-Like Flag
- koi_fpflag_ss: Stellar Eclipse Flag
- koi_fpflag_co: Centroid Offset Flag
- koi_fpflag_ec: Ephemeris Match Indicates Contamination Flag
- koi_tce_plnt_num: TCE Planet Number
- KIC parameter : 
  Kepler Input Catalog number assigned to a star that was targeted by the Kepler 
  Space Telescope mission. It serves as a unique identifier for stars observed by 
  Kepler, many of which have been found to host exoplanets. 
  - ra: KIC Right Ascension
  - dec: KIC Declination
  - koi_kepmag: Kepler-band

koi_model_snr: Transit Signal-to-Noise ?

We also don't take into account first the upper and lower bound of some features
We will include them in our analysis later

We choose to work on the class (confirmed, false positive) instead of
the score, so we remove it

We also remove the still unconfirmed candidate exoplanets because we
only work with the ones whose analyse is finished

```{r}

column_rm = c("kepid","kepoi_name","kepler_name","koi_pdisposition",
              "koi_score",
              "koi_teq_err1","koi_teq_err2","koi_tce_delivname")

column_rm = c("kepid","kepoi_name","kepler_name","koi_pdisposition",
              "koi_score",
              "koi_teq_err1","koi_teq_err2","koi_tce_delivname",
              "koi_fpflag_nt", "koi_fpflag_ss", "koi_fpflag_co", "koi_fpflag_ec",
              "ra", "dec", "koi_kepmag", 
              "koi_tce_plnt_num")

column_incertitudes = c("koi_period_err1", "koi_period_err2", "koi_time0bk_err1", "koi_time0bk_err2", 
                        "koi_impact_err1", "koi_impact_err2", "koi_duration_err1", "koi_duration_err2", 
                        "koi_depth_err1", "koi_depth_err2", "koi_prad_err1", "koi_prad_err2", 
                        "koi_insol_err1", "koi_insol_err2", "koi_steff_err1", "koi_steff_err2", 
                        "koi_slogg_err1", "koi_slogg_err2", "koi_srad_err1", "koi_srad_err2")

KOI_table <- KOI_table_full %>% dplyr::select(-all_of(column_rm))
KOI_table <- KOI_table %>% filter(koi_disposition != "CANDIDATE")
KOI_table.wo_incertitudes <- KOI_table %>% dplyr::select(-all_of(column_incertitudes))

real_columns_to_remove <- c("kepid", "kepoi_name", "kepler_name", "koi_pdisposition", "koi_score", 
                       "koi_fpflag_nt", "koi_fpflag_ss", "koi_fpflag_co", "koi_fpflag_ec", 
                       "ra", "dec", "koi_kepmag", "koi_model_snr", "koi_tce_plnt_num", 
                       "koi_tce_delivname")
```

We now have : two input space: one with and one without the incertitudes

```{r}
head(KOI_table)
dim(KOI_table)
names(KOI_table)
```

```{r}
head(KOI_table.wo_incertitudes)
dim(KOI_table.wo_incertitudes)
names(KOI_table.wo_incertitudes)
```
#### Class encoding

We encode binarly the class : 0 if false positive, 1 if confirmed
exoplanet

```{r}
KOI_table$koi_disposition <- ifelse(KOI_table$koi_disposition == "CONFIRMED", 1, 0)
unique(KOI_table$koi_disposition)
```
```{r}
KOI_table.wo_incertitudes$koi_disposition <- ifelse(KOI_table.wo_incertitudes$koi_disposition == "CONFIRMED", 1, 0)
unique(KOI_table.wo_incertitudes$koi_disposition)
```

### Data cleaning

#### NA values

Let's work on missing datapoint

```{r}
colSums(is.na(KOI_table))
```

We have different options : - remove NA - fill the NA with specific
value : mean, mice, etc...

If we remove NA, we have : We lose 653 observations in total

```{r}
KOI_table = na.omit(KOI_table)
KOI_table.wo_incertitudes = na.omit(KOI_table.wo_incertitudes)
dim(KOI_table)
dim(KOI_table.wo_incertitudes)
```

### Data analyse

```{r message=FALSE, warning=FALSE}
attach(KOI_table)
```

#### Class distribution

##### Class imbalance 

We can look for class imbalance : 
Class imbalance can biased the model towards the majority class 
- it can lead to poor recall or precision on the minority class 
- it can draw boundaries too close to the minority

```{r}
# Create a frequency table
table_koi <- table(KOI_table[["koi_disposition"]])
table_koi
# Convert to proportions
prop_koi <- prop.table(table_koi)
prop_koi
# Create a bar plot
barplot(prop_koi,
        col = rainbow(length(prop_koi)),  # Assign colors dynamically
        ylim = c(0,1),
        main = "Class Distribution",
        ylab = "Proportion",
        xlab = "KOI Disposition")
```
In our case, we can see imbalance between classes that we can handle
with for example : - Oversampling (SMOTE) or Undersampling

```{r small_plot, fig.height=3, fig.width=5, message=FALSE}
# Per class
for (feature in KOI_table){
  boxplot(feature ~ koi_disposition, data = KOI_table, main = "Boxplot by Class")
}
dim(koi_disposition)
```

```{r}
### Pairplot for some features
#pairs(KOI_table)
#selected_features <- KOI_table[, c("koi_fpflag_nt", "koi_period", "koi_duration", "koi_teq", #"koi_steff", "koi_disposition")]
#ggpairs(selected_features, aes(color = "koi_disposition"))
#pairs(selected_features, aes(color="koi_disposition"))
```

##### Solutions 

We have a moderate class imbalance : 
- class 0 (false positive) : 0.618%
- class 1 (true exoplanets) : 0.382%

We can use :
- up/down sampling: 
  Randomly duplicate examples from the minority class until both classes are equally represented
  Or Randomly remove examples from the majority class to match the minority.
- use class weights (for logistic regression or SVM):
  Assigns higher cost (importance) to errors made on the minority class.

#### Normality test

```{r}
n <- dim(KOI_table)
p <- dim(KOI_table)
```


#### Standardized data

Boxplots

```{r}
KOI_table_scaled <- KOI_table
KOI_table_scaled[, -which(names(KOI_table_scaled) == "koi_disposition")] <- scale(KOI_table_scaled[, -which(names(KOI_table_scaled) == "koi_disposition")])
boxplot(KOI_table_scaled, las = 2, main = "Standardized Features Boxplot")

KOI_table_scaled
```

#### Correlation

Correlation between features

```{r}
cor_matrix <- cor(KOI_table[, -which(names(KOI_table) == "koi_disposition")], use = "complete.obs")
corrplot(cor_matrix, method = "color", tl.cex = 0.6, tl.srt = 45)
```

Correlation between features and the class :

```{r}

```

#### Variance explained

##### PCA

We have a lot of features, PCA will help to reduce the dimension and
interpret the features

```{r}
pca <- prcomp(KOI_table[, -which(names(KOI_table) == "koi_disposition")], scale. = TRUE)
summary(pca)
```

```{r}
# Cumulative variance
var_explained <- pca$sdev^2 / sum(pca$sdev^2)
cumvar <- cumsum(var_explained)
plot(cumvar, 
     xlab = "Number of Principal Components", 
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b", pch = 19, col = "blue",
     main = "Cumulative Variance Explained by PCs")
abline(h = 0.9, col = "red", lty = 2)  # optional 90% guide
```

```{r}
# PCA loadings histogram
loadings <- as.data.frame(pca$rotation)  # Loadings matrix
loadings$Feature <- rownames(loadings)   # Feature names
loadings_long <- loadings %>% pivot_longer(cols = starts_with("PC"), names_to = "Principal_Component", values_to = "Loading")

ggplot(loadings_long %>% filter(Principal_Component == "PC1"), aes(x = reorder(Feature, Loading), y = Loading)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +  # Flip axes for readability
  theme_minimal() +
  labs(title = "PCA Loadings for PC1", x = "Features", y = "Loading Value")
```

First 2 principal components :

```{r}
# Select the first 2 principal components
df_pca <- data.frame(pca$x[, 1:2], label = koi_disposition)
df_pca
ggplot(df_pca, aes(x = PC1, y = PC2, color = label)) +
  geom_point() +
  theme_minimal()
```

First 3 principal components :

```{r}
# 3D Scatter Plot
df_pca <- data.frame(pca$x[, 1:3], label = koi_disposition)
plot3d(df_pca$PC1, df_pca$PC2, df_pca$PC3, 
       col = rainbow(length(unique(df_pca$label)))[as.factor(df_pca$label)],
       size = 5, 
       xlab = "PC1", ylab = "PC2", zlab = "PC3", 
       main = "3D PCA Visualization")
```

First 5 principal components :

```{r}
# Select the first 5 principal components
df_pca_5 <- data.frame(pca$x[, 1:5], label = koi_disposition)
# Parallel coordinate plot
ggparcoord(df_pca_5, columns = 1:5, groupColumn = "label", alphaLines = 0.5)
```

Entre parenthèse KOI_table_PCA = pca$x
On ne garde que les p premières composantes :
p = 20
Pour ..
cumsum(eigenvalues)[20]/tail(cumsum(eigenvalues),1)
% de la variance totale
KOI_table_PCA = pca$x[,1:20] dim(KOI_table_PCA)


#### Outliers Analysis & creation of new datasets 

```{r message=FALSE, warning=FALSE}
library(mice)        # Handling missing data
library(VIM)         # Visualizing missing data
```

##### Features distributions

Let's look at the boxplots of each quantitative variable to see if there seem to be outliers : 
```{r}
data = KOI_table
numeric_data <- data[sapply(data, is.numeric)]
# Loop through each numeric variable and plot a boxplot
for (feature in names(numeric_data)) {
  p <- ggplot(data, aes_string(y = feature)) +
    geom_boxplot(fill = "lightblue", outlier.colour = "red", outlier.shape = 1) +
    theme_minimal() +
    labs(title = paste("Boxplot of", feature), y = feature)
  
  print(p)
}
```

After looking at the feature boxplots, we saw a lot of potential outliers.

Example: srad above a threshold => only FP ?
```{r}
KOI_table[which(KOI_table$koi_srad>2),]$koi_disposition
```

##### What can we say about the outliers : 

let's look if the outliers are the same for every features : 
```{r}
data = KOI_table
data$koi_disposition = as.factor(data$koi_disposition)
numeric_cols <- names(data)[sapply(data, is.numeric)]
outlier_indices_list <- list()
indexes = 1:7132
```

```{r}
for (col in numeric_cols) {
  x <- data[[col]]
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR_val
  upper_bound <- Q3 + 1.5 * IQR_val
  outlier_indices_list[[col]] <- which(x < lower_bound | x > upper_bound)
  indexes = setdiff(indexes,which(x < lower_bound | x > upper_bound))
  
}
#outlier_indices_list
length(indexes)
```
If we remove all outliers we would keep only 2500 datapoints... 
we shouldn't discard the outliers 

##### Outliers in the errors
(units with very high error on a measurement)

let's see if they are associated with FP if not we should discard them
```{r}
error_cols <- c("koi_insol_err1", "koi_slogg_err1","koi_steff_err2","koi_steff_err1") 

for (col in error_cols) {
  p <- ggplot(data, aes_string(x = col)) +
    geom_histogram(bins = 50, fill = "skyblue", color = "black") +
    theme_minimal() +
    labs(title = paste("Distribution of", col), x = col, y = "Count")
  print(p)
}
```

check correlation with exoplanet : 
```{r}
for (col in error_cols) {
  p <- ggplot(data, aes_string(x = "koi_disposition", y = col)) +
    geom_boxplot(fill = "lightgreen") +
    theme_minimal() +
    labs(title = paste(col, "by Label"), x = "koi_disposition", y = col)
  print(p)
}
```
we see that higher errors are associated with false positive more often
We want to find a value to cap the errors, errors extremely high dont bring 
More info than high errors

```{r}
sum(data$koi_srad_err1>0.5 & data$koi_disposition==1)
sum(data$koi_srad_err1>0.5 & data$koi_disposition==0)
```
```{r}
ggplot(data, aes(x = koi_duration_err1, fill = koi_disposition)) +
  geom_histogram(bins = 100, position = "identity", alpha = 0.5) +
  labs(title = "Distribution of koi_duration_err1 by disposition") +
  theme_minimal()
```

We repeat for each Feature : 
```{r}
distrib_95 <- list()
features = numeric_cols
features_to_cap = features
# Loop through each feature
for (col in features) {
  val <- quantile(data[[col]], 0.95, na.rm = TRUE)
  
  n_1 <- sum(data[[col]] > val & data$koi_disposition == 1, na.rm = TRUE)
  n_0 <- sum(data[[col]] > val & data$koi_disposition == 0, na.rm = TRUE)
  prop = max(n_1,n_0)/(n_1+n_0)
  cat(prop)
  if(prop<0.7){
    features_to_cap = setdiff(features_to_cap,col)
  }
  distrib_95[[col]] <- list(
    above_95_class1 = n_1,
    above_95_class0 = n_0,
    prop_class = prop
  )
}
```
```{r}
length(features_to_cap)
```

for every feature, above a certain threshold there is only FP or Only Confirmed
=> We can cap the values of all the features that exhibit this pattern : data_partially_capped
=> we can cap all the features : data_capped


Computation of the capped dataset : 
```{r}
data_capped = data
for (col in features) {
  val <- quantile(data[[col]], 0.95, na.rm = TRUE)
  data_capped[[col]] <- pmin(data[[col]], val)
}
```

boxplots of the capped features : 
```{r}
numeric_data <- data[sapply(data, is.numeric)]
# Loop through each numeric variable and plot a boxplot
for (feature in names(numeric_data)) {
  p <- ggplot(data_capped, aes_string(y = feature)) +
    geom_boxplot(fill = "lightblue", outlier.colour = "red", outlier.shape = 1) +
    theme_minimal() +
    labs(title = paste("Boxplot of", feature), y = feature)
  
  print(p)
}
```

##### Limit influence of outliers

test with log(data) : 
```{r}
data_log =  log(abs(data[,-c(1)]))
summary(data_log)
```

We have problems with negative values
we can use log(1+x) : 
```{r}
data_log =  log1p(abs(data[,-c(1)]))

data_log$koi_disposition = data$koi_disposition
numeric_data <- data_log[sapply(data_log, is.numeric)]

# Loop through each numeric variable and plot a boxplot
for (feature in names(numeric_data)) {
  p <- ggplot(data_log, aes_string(y = feature)) +
    geom_boxplot(fill = "lightblue", outlier.colour = "red", outlier.shape = 1) +
    theme_minimal() +
    labs(title = paste("Boxplot of", feature), y = feature)
  
  print(p)
}
```

boxplots look better : except koi_slogg => we can cap koi_slogg
```{r}
f_to_cap = c("koi_slogg","koi_time0bk_err2","koi_time0bk_err1",
             "koi_period_err1","koi_period_err2")
for(col in f_to_cap){
  x = data_log[[col]]
  p5 <- quantile(x, 0.05, na.rm = TRUE)
  p95 <- quantile(x, 0.95, na.rm = TRUE)
  
  data_log[[col]] <- pmin(pmax(x, p5), p95)

}
```

```{r}
hist(data_log$koi_period_err1, breaks = 100)
```

we that this feature ( period error 1 and 2 ) still have a lot of outliers after 
log transform and capping, we can simply remove them 
since they are not very discriminative : data_log_f
```{r}
data_log_f = data_log[,-c(colnames(data_log)==c("koi_period_err2","koi_period_err1"))]
```
##### Resulting datasets

We now have the datasets : 
```{r}
# dataset obtained after performing a logtransform of the numerical features
# and capping some of those features exhibiting many outliers
KOI_table.log = data_log   
dim(KOI_table.log)

# after removing 2 uninformative features that have many outliers
KOI_table.log_f = data_log_f 
dim(KOI_table.log_f)

# after capping the values of the outliers at the 95% quantile 
KOI_table.capped = data_capped
dim(KOI_table.capped)

#### To obtain the datasets wo the incertitudes :
KOI_table.wo_incertitudes.log <- KOI_table.log %>% dplyr::select(-all_of(column_incertitudes))
KOI_table.wo_incertitudes.capped <- KOI_table.capped %>% dplyr::select(-all_of(column_incertitudes))
dim(KOI_table.wo_incertitudes.log)
dim(KOI_table.wo_incertitudes.capped)
# ... use this line to get any dataset wo incertitudes : KOI_table_capped %>% dplyr::select(-all_of(column_incertitudes))
```

### Classification

#### Weights for each observations 


For each observation we compute the weight according to the errors for each feature. 

For each feature the associated score is computed as : 
(abs(err1)+abs(ee2))/(2*abs(value of the feature))

then we average this score over each features. 
We obtain the final weight of an observation as : 
1/(1+avg(score per feature))

the higher the errors the lower the weight
```{r}
# List of main features and their corresponding error columns
features <- c("koi_period", "koi_time0bk", "koi_impact", "koi_duration", "koi_depth", 
              "koi_prad", "koi_insol", "koi_steff", "koi_slogg", "koi_srad")

upper_errs <- paste0(features, "_err1")
lower_errs <- paste0(features, "_err2")

# Function to compute relative uncertainty per feature for all rows
relative_uncertainty <- function(value, err1, err2) {
  abs_err_mean <- (abs(err1) + abs(err2)) / 2
  rel_unc <- abs_err_mean / abs(value)
  return(rel_unc)
}

# Initialize a matrix to store relative uncertainties for each feature
rel_unc_mat <- matrix(NA, nrow = nrow(KOI_table_clean), ncol = length(features))
colnames(rel_unc_mat) <- features

# Compute relative uncertainties for each feature
for (i in seq_along(features)) {
  val <- KOI_table_clean[[features[i]]]
  err1 <- KOI_table_clean[[upper_errs[i]]]
  err2 <- KOI_table_clean[[lower_errs[i]]]
  
  # Avoid division by zero or NA by replacing zeros with small value or NA handling
  val[val == 0] <- NA
  
  rel_unc_mat[, i] <- relative_uncertainty(val, err1, err2)
}

# Aggregate uncertainty across features per observation (mean of available relative uncertainties)
total_uncertainty <- apply(rel_unc_mat, 1, function(x) mean(x, na.rm = TRUE))

# Compute weights: higher weight for lower uncertainty
weights <- 1 / (1 + total_uncertainty)
# Normalize weights between 0 and 1
weights_norm <- weights / max(weights, na.rm = TRUE)
length(weights_norm)
```

The weights are stored in the vector weights_norm, lower weight for more uncertain observations. (vector of size 7132)


#### Split the data

```{r message=FALSE, warning=FALSE}
library(caret)
```

We divide the dataset into a : 
- training set 
- validation set
- testing set

##### Dataset with all features (with incertitudes)
```{r}
set.seed(123)  # For reproducibility

# Step 1: Split into train (70%) and temp (40%)
train_index <- createDataPartition(KOI_table$koi_disposition, p = 0.7, list = FALSE)
train_set <- KOI_table[train_index, ]
temp_set  <- KOI_table[-train_index, ]

# Step 2: Split temp into validation and test (15%)
val_index <- createDataPartition(temp_set$koi_disposition, p = 0.5, list = FALSE)
val_set <- temp_set[val_index, ]
test_set <- temp_set[-val_index, ]

train_set$koi_disposition <- as.factor(train_set$koi_disposition)
val_set$koi_disposition <- as.factor(val_set$koi_disposition)
test_set$koi_disposition <- as.factor(test_set$koi_disposition)

```

```{r}
head(train_set)
dim(train_set)
dim(val_set)
dim(test_set)

control <- trainControl(method = 'cv', number = 5)  # 5-fold CV
```

##### Dataset without incertitudes
```{r}
train_index.wo <- createDataPartition(KOI_table.wo_incertitudes$koi_disposition, p = 0.7, list = FALSE)
train_set.wo <- KOI_table.wo_incertitudes[train_index.wo, ]
temp_set.wo  <- KOI_table.wo_incertitudes[-train_index.wo, ]

val_index.wo <- createDataPartition(temp_set$koi_disposition, p = 0.5, list = FALSE)
val_set.wo <- temp_set.wo[val_index.wo, ]
test_set.wo <- temp_set.wo[-val_index.wo, ]

train_set.wo$koi_disposition <- as.factor(train_set.wo$koi_disposition)
val_set.wo$koi_disposition <- as.factor(val_set.wo$koi_disposition)
test_set.wo$koi_disposition <- as.factor(test_set.wo$koi_disposition)

```

##### Dataset without outliers

```{r}
# Step 1: Split into train (70%) and temp (40%)
train_index.log_f <- createDataPartition(KOI_table.log_f$koi_disposition, p = 0.7, list = FALSE)
train_set.log_f <- KOI_table[train_index.log_f, ]
temp_set.log_f  <- KOI_table[-train_index.log_f, ]

# Step 2: Split temp into validation and test (15%)
val_index.log_f <- createDataPartition(temp_set.log_f$koi_disposition, p = 0.5, list = FALSE)
val_set.log_f <- temp_set[val_index.log_f, ]
test_set.log_f <- temp_set[-val_index.log_f, ]

train_set.log_f$koi_disposition <- as.factor(train_set.log_f$koi_disposition)
val_set.log_f$koi_disposition <- as.factor(val_set.log_f$koi_disposition)
test_set.log_f$koi_disposition <- as.factor(test_set.log_f$koi_disposition)
```

##### Dataset without outliers and incertitudes

```{r}
train_index.wo.log <- createDataPartition(KOI_table.wo_incertitudes.log$koi_disposition, p = 0.7, list = FALSE)
train_set.wo.log <- KOI_table.wo_incertitudes.log[train_index.wo.log, ]
temp_set.wo.log  <- KOI_table.wo_incertitudes.log[-train_index.wo.log, ]

val_index.wo.log <- createDataPartition(temp_set.log$koi_disposition, p = 0.5, list = FALSE)
val_set.wo.log <- temp_set.log[val_index.wo.log, ]
test_set.wo.log <- temp_set.log[-val_index.wo.log, ]

train_set.wo.log$koi_disposition <- as.factor(train_set.wo.log$koi_disposition)
val_set.wo.log$koi_disposition <- as.factor(val_set.wo.log$koi_disposition)
test_set.wo.log$koi_disposition <- as.factor(test_set.wo.log$koi_disposition)
```

#### LDA

LDA can be robust without normality assumption

##### with outliers

###### with incertitudes

```{r}
train_set_wo_cluster <- train_set[, !(names(train_set) %in% "cluster")]
test_set_wo_cluster <- test_set[, !(names(test_set) %in% "cluster")]

lda_model1 <- lda(koi_disposition ~ ., data = train_set_wo_cluster)
lda_pred1 <- predict(lda_model, newdata = test_set_wo_cluster)


# Evaluate the model using a confusion matrix
confusionMatrix(lda_pred1$class, test_set_wo_cluster$koi_disposition)


# 2. Project all test set observations onto the discriminant axes
lda_proj1 <- predict(lda_model, newdata = test_set)

# 3. Create a data frame for ggplot
lda_df1 <- data.frame(lda_proj1$x)
lda_df1$koi_disposition <- test_set$koi_disposition

library(ggplot2)

# Plot the distribution of the classes along the first discriminant axis
ggplot(lda_df1, aes(x = LD1, fill = koi_disposition)) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of the classes along LD1",
       x = "Discriminant 1 (LD1)",
       y = "Density") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")

```

######without incertitudes

```{r message=FALSE, warning=FALSE}
library(MASS) 
library(caret)

# Train the model on the training set
lda_model2 <- lda(koi_disposition ~ ., data = train_set.wo)

# Predict directly on the test set
lda_pred2 <- predict(lda_model2, newdata = test_set.wo)

# Evaluate the model using a confusion matrix
confusionMatrix(lda_pred2$class, test_set.wo$koi_disposition)

# 1. Train the LDA model on the training set
lda_model2 <- lda(koi_disposition ~ ., data = train_set.wo)

# 2. Project all test set observations onto the discriminant axes
lda_proj2 <- predict(lda_model2, newdata = test_set.wo)

# 3. Create a data frame for ggplot
lda_df2 <- data.frame(lda_proj2$x)
lda_df2$koi_disposition <- test_set.wo$koi_disposition

library(ggplot2)

# Plot the distribution of the classes along the first discriminant axis
ggplot(lda_df2, aes(x = LD1, fill = koi_disposition)) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of the classes along LD1",
       x = "Discriminant 1 (LD1)",
       y = "Density") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")


```

##### without ouliers

###### with incertitudes
```{r}
library(MASS)
library(caret)
library(ggplot2)
library(RColorBrewer)

run_lda_analysis <- function(train_set, test_set, title = "LDA Projection") {
  # S'assurer que le facteur est bien défini
  train_set$koi_disposition <- as.factor(train_set$koi_disposition)
  test_set$koi_disposition <- as.factor(test_set$koi_disposition)
  
  # Entraînement
  lda_model <- lda(koi_disposition ~ ., data = train_set)
  
  # Prédiction
  lda_pred <- predict(lda_model, newdata = test_set)
  
  # Confusion matrix avec gestion des niveaux
  cat("\n=== Confusion Matrix:", title, "===\n")
  print(confusionMatrix(
    factor(lda_pred$class, levels = levels(train_set$koi_disposition)),
    factor(test_set$koi_disposition, levels = levels(train_set$koi_disposition))
  ))
  
  # Projection pour ggplot
  lda_df <- data.frame(lda_pred$x)
  lda_df$koi_disposition <- test_set$koi_disposition
  
  # Graphique
  p <- ggplot(lda_df, aes(x = LD1, fill = koi_disposition)) +
    geom_density(alpha = 0.5) +
    labs(title = title,
         x = "LD1", y = "Density") +
    theme_minimal() +
    scale_fill_brewer(palette = "Set1")
  
  print(p)
}

run_lda_analysis(train_set.log_f, test_set.log_f, "LDA Projection - log_f (no outliers)")


```

###### without incertitudes

```{r}
run_lda_analysis(train_set.wo.log, test_set.wo.log, "LDA Projection - log (no uncertainties)")
```


#### KNN

Let's try K-Nearest Neighbor method :

##### With incertitudes
```{r}
library(class) # KNN
```

```{r}
knn.all <- train(
  koi_disposition ~ ., 
  data = train_set,
  method = "knn",
  preProcess = c("center", "scale"),  # normalize predictors
  tuneLength = 10  # try different values of k (number of neighbors)
)
```

Results:
```{r}
print(knn.all)
plot(knn.all)
```
```{r}
knn_preds <- predict(knn.all, newdata = val_set)
confusionMatrix(knn_preds, val_set$koi_disposition)
```
We can see very good prediction score using KNN.
Given the high dimensional space, we still have enough data to keep the distances meaningful.

KNN doesn't have internal coefficients (it's non-parametric), so we can't extract direct weights. 
But we can use model-agnostic methods like:
```{r warning=FALSE}
library(vip)
vip(knn.all)
```

##### Without incertitudes

```{r}
knn.wo_incertitudes <- train(
  koi_disposition ~ ., 
  data = train_set.wo,
  method = "knn",
  preProcess = c("center", "scale"),  # normalize predictors
  tuneLength = 10  # try different values of k (number of neighbors)
)
```

Results:
```{r}
print(knn.wo_incertitudes)
plot(knn.wo_incertitudes)
```
```{r}
knn_preds_wo <- predict(knn.wo_incertitudes, newdata = val_set.wo)
confusionMatrix(knn_preds_wo, val_set.wo$koi_disposition)
```


```{r}
vip(knn.wo_incertitudes)
```

##### Without outliers

#### SVM

##### with outliers

###### With incertitudes

```{r warning=FALSE}
library(e1071)
```
```{r}
svm <- train(
  koi_disposition ~ .,
  data = train_set,
  method = "svmLinear",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(C = c(0.01, 0.1, 1, 10, 100))  # define multiple values of C
)
```
```{r}
print(svm)
plot(svm)  # plots accuracy vs cost (C) values
```
```{r}
svm_preds <- predict(svm, newdata = val_set)
confusionMatrix(svm_preds, val_set$koi_disposition)
```
We got:
- accuracy: 92%
- recall: 92%
- specificity: 91%

```{r}
varImp(svm)
```

incertitudes are prevalent for prediction
then we have impact, prad, teq, insol

###### Without incertitudes

```{r}
svm.wo <- train(
  koi_disposition ~ .,
  data = train_set.wo,
  method = "svmLinear",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(C = c(0.01, 0.1, 1, 10, 100))  # define multiple values of C
)
```
```{r}
print(svm.wo)
plot(svm.wo)  # plots accuracy vs cost (C) values

```

```{r}
svm_preds_wo <- predict(svm.wo, newdata = val_set.wo)
confusionMatrix(svm_preds_wo, val_set.wo$koi_disposition)
```
Accuracy : 85%
Recall : 83%
Specificity : 88%

```{r}
varImp(svm.wo)
```

Order of importance :

1. koi_impact : Impact Parameter [Transit Property]
The sky-projected distance between the center of the stellar disc and the center 
of the planet disc at conjunction, normalized by the stellar radius.

2. koi_prad : Planetary Radius (Earth radius)  [Transit Property]
The radius of the planet. Planetary radius is the product of the planet star radius ratio and the stellar radius.

3. koi_teq : Equilibrium Temperature (Kelvin)  [Transit Property]
Approximation for the temperature of the planet

4. koi_insol : Insolation Flux [Earth flux]  [Transit Property]
Another way to give the equilibrium temperature
 
5. koi_steff : Stellar Effective Temperature (Kelvin)  [Stellar Parameter]

6. koi_time0bk : Transit Epoch  [Transit Property]
The time corresponding to the center of the first detected transit in Barycentric 
Julian Day (BJD) minus a constant offset of 2,454,833.0 days

etc...

##### without outliers

###### with incertitudes

```{r}
svm.log_f <- train(
  koi_disposition ~ .,
  data = train_set.log_f,
  method = "svmLinear",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(C = c(0.01, 0.1, 1, 10, 100))  # define multiple values of C
)
```
```{r}
print(svm.log_f)
plot(svm.log_f)  # plots accuracy vs cost (C) values
```
```{r}
svm_preds.log_f <- predict(svm.log_f, newdata = val_set.log_f)
confusionMatrix(svm_preds.log_f, val_set.log_f$koi_disposition)
```
We got:
- accuracy: 92%
- recall: 93%
- specificity: 89%

```{r}
varImp(svm.log_f)
```

###### without incertitudes

```{r}
svm.wo.log <- train(
  koi_disposition ~ .,
  data = train_set.wo.log,
  method = "svmLinear",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(C = c(0.01, 0.1, 1, 10, 100))  # define multiple values of C
)
```
```{r}
print(svm.wo.log)
plot(svm.wo.log)  # plots accuracy vs cost (C) values

```

```{r}
svm_preds.wo.log <- predict(svm.wo.log, newdata = val_set.wo.log)
confusionMatrix(svm_preds.wo.log, val_set.wo.log$koi_disposition)
```


```{r}
varImp(svm.wo.log)
```

##### Results SVM



#### Decision Tree

##### With incertitudes
```{r}
library(rpart)
tree_model <- rpart(koi_disposition ~ ., data = train_set, method = "class")
summary(tree_model)
tree_pred <- predict(tree_model, newdata = val_set, type = "class")
table(Predicted = tree_pred, Actual = val_set$koi_disposition)
```
```{r}
library(rpart.plot)
rpart.plot(tree_model)
```
error: 9.8%, accuracy: 90,18%
important variables: koi_staff_err1, koi_prad, koi_duration_err1

##### Without incertitudes
```{r}
library(rpart)
tree_model_wo <- rpart(koi_disposition ~ ., data = train_set.wo, method = "class")
summary(tree_model_wo)
tree_pred_wo <- predict(tree_model_wo, newdata = val_set.wo, type = "class")
table(Predicted = tree_pred_wo, Actual = val_set.wo$koi_disposition)
```
```{r}
library(rpart.plot)
rpart.plot(tree_model_wo)
```
error: 14.2%, accuracy: 85,98%
important variables: koi_prad, koi_model_snr

##### With data_log_f
```{r}
tree_model_log_f <- rpart(koi_disposition ~ ., data = train_set.log_f, method = "class")
summary(tree_model_log_f)
tree_pred_log_f <- predict(tree_model_log_f, newdata = val_set.log_f, type = "class")
table(Predicted = tree_pred_log_f, Actual = val_set.log_f$koi_disposition)
```
```{r}
rpart.plot(tree_model_log_f)
```
error: 8,59%, accuracy: 91,40%
important variables: koi_staff_err1, koi_prad

##### Without incertitudes_data_log
```{r}
tree_model_wo_log <- rpart(koi_disposition ~ ., data = train_set.wo_log, method = "class")
summary(tree_model_wo_log)
tree_pred_wo_log <- predict(tree_model_wo_log, newdata = val_set.wo_log, type = "class")
table(Predicted = tree_pred_wo_log, Actual = val_set.wo_log$koi_disposition)
```
```{r}
rpart.plot(tree_model_wo_log)
```
error: 14,00%, accuracy: 85,98%
important variables: koi_prad, koi_model_snr

##### Accuracy and comparison of the decision trees
```{r}
# Accuracy 
acc_tree <- mean(tree_pred == val_set$koi_disposition)
acc_tree_wo <- mean(tree_pred_wo == val_set.wo$koi_disposition)
acc_tree_log_f <- mean(tree_pred_log_f == val_set.log_f$koi_disposition)
acc_tree_wo_log <- mean(tree_pred_wo_log == val_set.wo_log$koi_disposition)
 
# comparison table
accuracy_table <- data.frame(
  model = c("with incertitudes", "without incertitudes", "with data_log_f", "with incertitudes__log"),
  accuracy = c(acc_tree, acc_tree_wo, acc_tree_log_f, acc_tree_wo_log)
)
print(accuracy_table)
```

```{r}
library(caret)

# Calcola confusion matrix e metriche per ogni modello
cm1 <- confusionMatrix(tree_pred,        reference = val_set$koi_disposition)
cm2 <- confusionMatrix(tree_pred_wo,     reference = val_set.wo$koi_disposition)
cm3 <- confusionMatrix(tree_pred_log_f,  reference = val_set.log_f$koi_disposition)
cm4 <- confusionMatrix(tree_pred_wo_log, reference = val_set.wo_log$koi_disposition)

# Estrai balanced accuracy, precision, recall, F1
get_metrics <- function(cm) {
  c(
    Accuracy = cm$overall["Accuracy"],
    Balanced_Accuracy = cm$byClass["Balanced Accuracy"],
    Precision = cm$byClass["Precision"],
    Recall = cm$byClass["Recall"],
    F1 = cm$byClass["F1"]
  )
}

metrics1 <- get_metrics(cm1)
metrics2 <- get_metrics(cm2)
metrics3 <- get_metrics(cm3)
metrics4 <- get_metrics(cm4)

comparison_table <- data.frame(
  Model = c("with incertitudes", "without incertitudes", "with data_log_f", "with incertitudes_log"),
  Accuracy = c(metrics1["Accuracy.Accuracy"], metrics2["Accuracy.Accuracy"], metrics3["Accuracy.Accuracy"], metrics4["Accuracy.Accuracy"]),
  Balanced_Accuracy = c(metrics1["Balanced_Accuracy.Balanced Accuracy"], metrics2["Balanced_Accuracy.Balanced Accuracy"], metrics3["Balanced_Accuracy.Balanced Accuracy"], metrics4["Balanced_Accuracy.Balanced Accuracy"]),
  Precision = c(metrics1["Precision.Precision"], metrics2["Precision.Precision"], metrics3["Precision.Precision"], metrics4["Precision.Precision"]),
  Recall = c(metrics1["Recall.Recall"], metrics2["Recall.Recall"], metrics3["Recall.Recall"], metrics4["Recall.Recall"]),
  F1 = c(metrics1["F1.F1"], metrics2["F1.F1"], metrics3["F1.F1"], metrics4["F1.F1"])
)
print(comparison_table)
```

#### Random forest

##### with incertitudes
```{r}
library(doParallel)
cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
library(caret)
control <- trainControl(method = "cv", number = 5)
rf_model <- train(koi_disposition ~ .,  data = train_set, method = "rf", trControl = control, metric = 'Accuracy',
            tuneGrid = data.frame(mtry = floor(sqrt(ncol(train_set) - 1))), ntree = 100)
print(rf_model$finalModel)
stopCluster(cl)
```
```{r}
## get variable importance , and turn into a data frame
var_imp <- varImp(rf_model, scale = FALSE)$importance
var_imp <- data.frame(variable = row.names(var_imp), importance = var_imp$Overall)
ggplot(var_imp %>% arrange(importance), aes(x = reorder(variable, importance), y = importance)) + 
  geom_bar(stat = 'identity') + 
  coord_flip() +
  xlab('Variables') +
  labs(title = 'Random Forest Variable Importance') + 
  theme_minimal() +
  theme(
    axis.text = element_text(size = 10), 
    axis.title = element_text(size = 15), 
    plot.title = element_text(size = 20)
  )
```
koi_fpflag_co (centroid offset flag) is the most important feature, followed by koi_fpflag_ss (stellar eclipse flag)
```{r}
## generate prediction and print the accuracy
rf_preds <- predict(rf_model, newdata = val_set)
rf_preds <- factor(rf_preds, levels = levels(val_set$koi_disposition))
accuracy <- mean(rf_preds == val_set$koi_disposition)*100
cat('Accuracy on val_set: ', round(accuracy, 2), '%', sep = '')
print(confusionMatrix(rf_preds, val_set$koi_disposition))
```
accuracy: 98.75%

##### without incertitudes
```{r}
library(doParallel)
cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
library(caret)
control_wo <- trainControl(method = "cv", number = 5)
rf_model_wo <- train(koi_disposition ~ .,  data = train_set.wo, method = "rf", trControl = control_wo, metric = 'Accuracy',
            tuneGrid = data.frame(mtry = floor(sqrt(ncol(train_set.wo) - 1))), ntree = 100)
print(rf_model_wo$finalModel)
stopCluster(cl)
```
```{r}
## get variable importance , and turn into a data frame
var_imp_wo <- varImp(rf_model_wo, scale = FALSE)$importance
var_imp_wo <- data.frame(variable=row.names(var_imp_wo), importance = var_imp_wo$Overall)
ggplot(var_imp_wo %>% arrange(importance), aes(x = reorder(variable, importance), y = importance)) + 
  geom_bar(stat = 'identity') + 
  coord_flip() +
  xlab('Variables') +
  labs(title = 'Random Forest Variable Importance') + 
  theme_minimal() +
  theme(
    axis.text = element_text(size = 10), 
    axis.title = element_text(size = 15), 
    plot.title = element_text(size = 20)
  )
```
koi_prad is the most important feature, followed by koi_model_snr
```{r}
## generate prediction and print the accuracy
rf_preds_wo <- predict(rf_model_wo, newdata = val_set.wo)
rf_preds_wo <- factor(rf_preds_wo, levels = levels(val_set.wo$koi_disposition))
accuracy <- mean(rf_preds_wo == val_set.wo$koi_disposition)*100
cat('Accuracy on val_set: ', round(accuracy, 2), '%', sep = '')
print(confusionMatrix(rf_preds_wo, val_set.wo$koi_disposition))
```
accuracy: 90.84%

##### without data_log_f
```{r}
library(doParallel)
cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
library(caret)
control_log_f <- trainControl(method = "cv", number = 5)
rf_model_log_f <- train(koi_disposition ~ .,  data = train_set.log_f, method = "rf", trControl = control_log_f, metric = 'Accuracy',
            tuneGrid = data.frame(mtry = floor(sqrt(ncol(train_set.log_f) - 1))), ntree = 100)
print(rf_model_log_f$finalModel)
stopCluster(cl)
```
```{r}
## get variable importance , and turn into a data frame
var_imp_log_f <- varImp(rf_model_log_f, scale = FALSE)$importance
var_imp_log_f <- data.frame(variable=row.names(var_imp_log_f), importance = var_imp_log_f$Overall)
ggplot(var_imp_log_f %>% arrange(importance), aes(x = reorder(variable, importance), y = importance)) + 
  geom_bar(stat = 'identity') + 
  coord_flip() +
  xlab('Variables') +
  labs(title = 'Random Forest Variable Importance') + 
  theme_minimal() +
  theme(
    axis.text = element_text(size = 10), 
    axis.title = element_text(size = 15), 
    plot.title = element_text(size = 20)
  )
```
koi_fpflag_co (centroid offset flag) is still the most important feature, with a greater weight
```{r}
## generate prediction and print the accuracy
rf_preds_log_f <- predict(rf_model_log_f, newdata = val_set.log_f)
rf_preds_log_f <- factor(rf_model_log_f, levels = levels(val_set.log_f$koi_disposition))
accuracy <- mean(rf_preds_log_f == val_set.log_f$koi_disposition)*100
cat('Accuracy on val_set: ', round(accuracy, 2), '%', sep = '')
print(confusionMatrix(rf_preds_log_f, val_set.log_f$koi_disposition))
```

##### without incertitudes_data_log
```{r}
library(doParallel)
cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
library(caret)
control_wo_log <- trainControl(method = "cv", number = 5)
rf_model_wo_log <- train(koi_disposition ~ .,  data = train_set.wo_log, method = "rf", trControl = control_log_f, metric = 'Accuracy',
            tuneGrid = data.frame(mtry = floor(sqrt(ncol(train_set.wo_log) - 1))), ntree = 100)
print(rf_model_wo_log$finalModel)
stopCluster(cl)
```
```{r}
## get variable importance , and turn into a data frame
var_imp_wo_log <- varImp(rf_model_wo_log, scale = FALSE)$importance
var_imp_wo_log <- data.frame(variable=row.names(var_imp_wo_log), importance = var_imp_wo_log$Overall)
ggplot(var_imp_wo_log %>% arrange(importance), aes(x = reorder(variable, importance), y = importance)) + 
  geom_bar(stat = 'identity') + 
  coord_flip() +
  xlab('Variables') +
  labs(title = 'Random Forest Variable Importance') + 
  theme_minimal() +
  theme(
    axis.text = element_text(size = 10), 
    axis.title = element_text(size = 15), 
    plot.title = element_text(size = 20)
  )
```
koi_prad is the most important feature, followed by koi_model_snr
```{r}
rf_preds_wo_log <- predict(rf_model_wo_log, newdata = val_set.wo_log)
rf_preds_wo_log <- factor(rf_preds_wo_log, levels = levels(val_set.wo_log$koi_disposition))
accuracy <- mean(rf_preds_wo_log == val_set.wo_log$koi_disposition) * 100
cat('Accuracy on val_set: ', round(accuracy, 2), '%', sep = '')
print(confusionMatrix(rf_preds_wo_log, val_set.wo_log$koi_disposition))
```
accuracy: 90.56%
#### Logistic regression

##### With incertitudes

###### Without class weights

Fitting the model on the dataset
```{r}
glm.fit <- glm(koi_disposition~., data = train_set, family=binomial)
summary(glm.fit)
```
Prediction on the testing set
```{r}
# Prediction on validation set
val_probs <- predict(glm.fit, newdata = val_set, type = "response")
# Class prediction
val_pred <- ifelse(val_probs > 0.5, 1, 0)
# Confusion matrix
table(Predicted = val_pred, Actual = val_set$koi_disposition)
# Accuracy
actual <- ifelse(val_set$koi_disposition == 0, 0, 1)
mean(val_pred == actual)
```
```{r}
library(pROC)
```
```{r}
roc_obj <- roc(val_set$koi_disposition, val_probs)
auc(roc_obj)
plot(roc_obj)
```

###### With class weights

Assign weights inversely proportional to class frequencies
```{r}
prop.table(table(train_set$koi_disposition))
class_weights <- ifelse(train_set$koi_disposition == 0,
                        1 / prop.table(table(train_set$koi_disposition))[1],
                        1 / prop.table(table(train_set$koi_disposition))[2])
```

```{r}
glm_weighted.fit <- glm(koi_disposition~., 
                        data = train_set, 
                        family=binomial,
                        weights = class_weights)
summary(glm_weighted.fit)
```
```{r}
val_probs_weighted <- predict(glm_weighted.fit, newdata = val_set, type = "response")
# Class prediction
val_pred_weighted <- ifelse(val_probs_weighted > 0.5, 1, 0)
# Confusion matrix
table(Predicted = val_pred_weighted, Actual = val_set$koi_disposition)
table(Predicted = val_pred, Actual = val_set$koi_disposition)
# Accuracy
actual_weighted <- ifelse(val_set$koi_disposition == 0, 0, 1)
mean(val_pred == actual_weighted)
mean(val_pred == actual)
```

##### Without incertitudes

###### Without class weights

Fitting the model on the dataset
```{r}
glm.fit.wo_incertitudes <- glm(koi_disposition~., data = train_set.wo, family=binomial)
summary(glm.fit.wo_incertitudes)
```
Prediction on the testing set
```{r}
# Prediction on validation set
val_probs.wo <- predict(glm.fit.wo_incertitudes, newdata = val_set.wo, type = "response")
# Class prediction
val_pred.wo <- ifelse(val_probs.wo > 0.5, 1, 0)
# Confusion matrix
table(Predicted = val_pred.wo, Actual = val_set.wo$koi_disposition)
# Accuracy
actual.wo <- ifelse(val_set.wo$koi_disposition == 0, 0, 1)
mean(val_pred.wo == actual.wo)
```

###### With class weights

```{r}
prop.table(table(train_set.wo$koi_disposition))
class_weights <- ifelse(train_set.wo$koi_disposition == 0,
                        1 / prop.table(table(train_set.wo$koi_disposition))[1],
                        1 / prop.table(table(train_set.wo$koi_disposition))[2])

glm_weighted.fit.wo_incertitudes <- glm(koi_disposition~., 
                                        data = train_set.wo, 
                                        family=binomial,
                                        weights = class_weights)

summary(glm_weighted.fit.wo_incertitudes)
```
```{r}
val_probs_weighted.wo <- predict(glm_weighted.fit.wo_incertitudes, newdata = val_set.wo, type = "response")
# Class prediction
val_pred_weighted.wo <- ifelse(val_probs_weighted.wo > 0.5, 1, 0)
# Confusion matrix
table(Predicted = val_pred_weighted.wo, Actual = val_set.wo$koi_disposition)
table(Predicted = val_pred.wo, Actual = val_set.wo$koi_disposition)
# Accuracy
actual_weighted.wo <- ifelse(val_set.wo$koi_disposition == 0, 0, 1)
mean(val_pred_weighted.wo == actual_weighted.wo)
mean(val_pred.wo == actual.wo)
```

##### Without outliers

###### Without class weights
Fitting the model on the dataset
```{r}
glm.fit.log_f <- glm(koi_disposition~., data = train_set.log_f, family=binomial)
summary(glm.fit.log_f)
```

Prediction on the testing set
```{r}
# Prediction on validation set
val_probs.log_f <- predict(glm.fit.log_f, newdata = val_set.log_f, type = "response")
# Class prediction
val_pred.log_f <- ifelse(val_probs.log_f > 0.5, 1, 0)
# Confusion matrix
table(Predicted = val_pred.log_f, Actual = val_set.log_f$koi_disposition)
# Accuracy
actual.log_f <- ifelse(val_set.log_f$koi_disposition == 0, 0, 1)
mean(val_pred.log_f == actual.log_f)
```

###### With class weights

```{r}
prop.table(table(train_set.log_f$koi_disposition))
class_weights <- ifelse(train_set.wo$koi_disposition == 0,
                        1 / prop.table(table(train_set.log_f$koi_disposition))[1],
                        1 / prop.table(table(train_set.log_f$koi_disposition))[2])

glm_weighted.fit.log_f <- glm(koi_disposition~., data = train_set.log_f, family=binomial)
summary(glm_weighted.fit.log_f)
```

Prediction on the testing set
```{r}
# Prediction on validation set
val_probs_weighted.log_f <- predict(glm_weighted.fit.log_f, newdata = val_set.log_f, type = "response")
# Class prediction
val_pred_weighted.log_f <- ifelse(val_probs_weighted.log_f > 0.5, 1, 0)
# Confusion matrix
table(Predicted = val_pred_weighted.log_f, Actual = val_set.log_f$koi_disposition)
# Accuracy
actual.log_f <- ifelse(val_set.log_f$koi_disposition == 0, 0, 1)
mean(val_pred_weighted.log_f == actual.log_f)
```

##### Results logistic regression

```{r}
table(Predicted = val_pred, Actual = val_set$koi_disposition)
mean(val_pred == actual)

table(Predicted = val_pred.wo, Actual = val_set.wo$koi_disposition)
mean(val_pred.wo == actual.wo)

table(Predicted = val_pred.log_f, Actual = val_set.log_f$koi_disposition)
mean(val_pred.log_f == actual.log_f)
```

#### Comparison between models

We use test set to evaluate the best model:

```{r}

```








### Clustering

```{r}
class <- train_set[1]
features <- train_set[-1]
```

#### Hiearchical clustering
```{r}
library(cluster)
```

Compute the dissimilarity matrix
```{r}
D.e <- dist(features,method="euclidean") 
D.m <- dist(features,method="manhattan") 
D.c <- dist(features,method="canberra") 
```

Testing multiple linkage
```{r}
D.es <- hclust(D.e, method = 'single')    # closest pair of points
D.ea <- hclust(D.e, method = 'average')   # average distance between all points
D.ec <- hclust(D.e, method = 'complete')  # farthest pair of points
D.ew <- hclust(D.e, method = 'ward.D2')   # minimize increase in total within-cluster variance
```

Plot dendograms
```{r}
plot(
  D.es,
  main = 'euclidean-single',
  hang = -0.1,
  xlab = '',
  labels = F,
  cex = 0.6,
  sub = ''
)
rect.hclust(D.es, k = 2)   # 2 clusters
plot(
  D.ec,
  main = 'euclidean-complete',
  hang = -0.1,
  xlab = '',
  labels = F,
  cex = 0.6,
  sub = ''
)
rect.hclust(D.ec, k = 2)
plot(
  D.ea,
  main = 'euclidean-average',
  hang = -0.1,
  xlab = '',
  labels = F,
  cex = 0.6,
  sub = ''
)
rect.hclust(D.ea, k = 2)
```
```{r}
# Cut tree into k clusters (e.g., k = 3)
cluster_labels <- cutree(D.es, k = 2)
cluster_labels
length(which(cluster_labels==TRUE))
length(cluster_labels==FALSE)
```

```{r}
# Add cluster assignments to your data
train_set$cluster <- as.factor(cluster_labels)
table(train_set$cluster)
pca <- prcomp(features, scale. = TRUE)
plot(pca$x[,1], pca$x[,2],
     col = cluster_labels,
     pch = 19,
     xlab = "PC1", ylab = "PC2",
     main = "PCA of Features Colored by Hierarchical Clusters")
legend("topright", legend = paste("Cluster", 1:2), col = 1:2, pch = 19)
```


```{r}
library(ggplot2)

pca_df <- as.data.frame(pca$x[, 1:2])
pca_df$cluster <- as.factor(cluster_labels)

ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(size = 1.8) +
  labs(title = "PCA Colored by Clusters", color = "Cluster") +
  theme_minimal()

```

#### K-means

```{r}

```

#### DBScan

```{r}

```

### Incertitudes influence

```{r}

```
