---
title: "Exoplanet"
output:
  html_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

### KOI_table

Dataset from the NASA : source :
<https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=cumulative>

```{r}
KOI_table_full = read.csv("KOI_table_2025.03.23_01.50.59.csv",comment.char = "#",header = TRUE, stringsAsFactors = FALSE)
```

### Librairies

```{r message=FALSE}
library(gitcreds) # git
library(dplyr) # selection
library(GGally) # pair plots
library(corrplot) # correlation heatmap
library(ggplot2) # box plots
library(rgl) # 3D plot
```

### Data exploration

#### Exploration

First of all, we begin by looking into our dataset (features, dimension, type)

```{r}
head(KOI_table_full)
```

```{r}
names(KOI_table_full)
```

```{r}
str(KOI_table_full)
```

```{r}
summary(KOI_table)
```

```{r}
dim(KOI_table_full)
class(KOI_table_full)
```
#### Input space

We have to remove the unnecessary, unrelevant columns which are : 
(data that are not relevant from a scientific, astronomic point of view)
- object id 
- object name 
- satellite name 
- pdisposition (Kepler) (less information than disposition) 
- koi_depth_err1: Transit Depth Upper Unc. [ppm] (no information : NA) 
- koi_depth_err2: Transit Depth Lower Unc. [ppm] (no information : NA) 
- koi_tce_delivname: TCE Delivery (it's just the delivery type) 

We also don't take into account first the upper and lower bound of some features
We will include them in our analysis later

We choose to work on the class (confirmed, false positive) instead of
the score, so we remove it

We also remove the still unconfirmed candidate exoplanets because we
only work with the ones whose analyse is finished

```{r}
column_rm = c("kepid","kepoi_name","kepler_name","koi_pdisposition",
              "koi_score",
              "koi_teq_err1","koi_teq_err2","koi_tce_delivname")

column_incertitudes = c("koi_period_err1", "koi_period_err2", "koi_time0bk_err1", "koi_time0bk_err2", 
                        "koi_impact_err1", "koi_impact_err2", "koi_duration_err1", "koi_duration_err2", 
                        "koi_depth_err1", "koi_depth_err2", "koi_prad_err1", "koi_prad_err2", 
                        "koi_insol_err1", "koi_insol_err2", "koi_steff_err1", "koi_steff_err2", 
                        "koi_slogg_err1", "koi_slogg_err2", "koi_srad_err1", "koi_srad_err2")

KOI_table <- KOI_table_full %>% select(-all_of(column_rm))
KOI_table <- KOI_table %>% filter(koi_disposition != "CANDIDATE")

KOI_table_wo_incertitudes <- KOI_table %>% select(-all_of(column_incertitudes))

real_columns_to_remove <- c("kepid", "kepoi_name", "kepler_name", "koi_pdisposition", "koi_score", 
                       "koi_fpflag_nt", "koi_fpflag_ss", "koi_fpflag_co", "koi_fpflag_ec", 
                       "ra", "dec", "koi_kepmag", "koi_model_snr", "koi_tce_plnt_num", 
                       "koi_tce_delivname")

```

We now have : two input space: one with and one without the incertitudes

```{r}
head(KOI_table)
dim(KOI_table)
names(KOI_table)
```

```{r}
head(KOI_table_wo_incertitudes)
dim(KOI_table_wo_incertitudes)
names(KOI_table_wo_incertitudes)
```
#### Class encoding

We encode binarly the class : 0 if false positive, 1 if confirmed
exoplanet

```{r}
KOI_table$koi_disposition <- ifelse(KOI_table$koi_disposition == "CONFIRMED", 1, 0)
unique(KOI_table$koi_disposition)
```

### Data cleaning

#### NA values

Let's work on missing datapoint

```{r}
colSums(is.na(KOI_table))
```

We have different options : - remove NA - fill the NA with specific
value : mean, mice, etc...

If we remove NA, we have : We lose 653 observations in total

```{r}
KOI_table = na.omit(KOI_table)
KOI_table_wo_incertitudes = na.omit(KOI_table_wo_incertitudes)
dim(KOI_table)
dim(KOI_table_wo_incertitudes)
```

### Data analyse

```{r}
attach(KOI_table)
```

#### Class distribution

We can look for class imbalance : Class imbalance can biased the model
towards the majority class - it can lead to poor recall or precision on
the minority class - it can draw boundaries too close to the minority
class (Log Reg, SVM)

```{r}
# Create a frequency table
table_koi <- table(KOI_table[["koi_disposition"]])
table_koi
# Convert to proportions
prop_koi <- prop.table(table_koi)
# Create a bar plot
barplot(prop_koi,
        col = rainbow(length(prop_koi)),  # Assign colors dynamically
        ylim = c(0,1),
        main = "Class Distribution",
        ylab = "Proportion",
        xlab = "KOI Disposition")
```

In our case, we can see imbalance between classes that we can handle
with for example : - Oversampling (SMOTE) or Undersampling

A revoir !

```{r small_plot, fig.height=3, fig.width=5, message=FALSE}
# Per class
for (feature in KOI_table){
  boxplot(feature ~ koi_disposition, data = KOI_table, main = "Boxplot by Class")
}
dim(koi_disposition)
```

A revoir !

```{r}
### Pairplot for some features
#pairs(KOI_table)
#selected_features <- KOI_table[, c("koi_fpflag_nt", "koi_period", "koi_duration", "koi_teq", #"koi_steff", "koi_disposition")]
#ggpairs(selected_features, aes(color = "koi_disposition"))
#pairs(selected_features, aes(color="koi_disposition"))
```

#### Features distributions





##### Normality test

```{r}
n <- dim(KOI_table)
p <- dim(KOI_table)
```



##### Standardized data

Boxplots

```{r}
KOI_table_scaled <- KOI_table
KOI_table_scaled[, -which(names(KOI_table_scaled) == "koi_disposition")] <- scale(KOI_table_scaled[, -which(names(KOI_table_scaled) == "koi_disposition")])
boxplot(KOI_table_scaled, las = 2, main = "Standardized Features Boxplot")

KOI_table_scaled
```

#### Correlation

Correlation between features

```{r}
cor_matrix <- cor(KOI_table[, -which(names(KOI_table) == "koi_disposition")], use = "complete.obs")
corrplot(cor_matrix, method = "color", tl.cex = 0.6, tl.srt = 45)
```

Correlation between features and the class :

```{r}

```

#### Variance explained

##### PCA

We have a lot of features, PCA will help to reduce the dimension and
interpret the features

```{r}
pca <- prcomp(KOI_table[, -which(names(KOI_table) == "koi_disposition")], scale. = TRUE)
summary(pca)
```

```{r}
# Cumulative variance
var_explained <- pca$sdev^2 / sum(pca$sdev^2)
cumvar <- cumsum(var_explained)
plot(cumvar, 
     xlab = "Number of Principal Components", 
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b", pch = 19, col = "blue",
     main = "Cumulative Variance Explained by PCs")
abline(h = 0.9, col = "red", lty = 2)  # optional 90% guide
```

```{r}
# PCA loadings histogram
loadings <- as.data.frame(pca$rotation)  # Loadings matrix
loadings$Feature <- rownames(loadings)   # Feature names
loadings_long <- loadings %>% pivot_longer(cols = starts_with("PC"), names_to = "Principal_Component", values_to = "Loading")

ggplot(loadings_long %>% filter(Principal_Component == "PC1"), aes(x = reorder(Feature, Loading), y = Loading)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +  # Flip axes for readability
  theme_minimal() +
  labs(title = "PCA Loadings for PC1", x = "Features", y = "Loading Value")
```

First 2 principal components :

```{r}
# Select the first 2 principal components
df_pca <- data.frame(pca$x[, 1:2], label = koi_disposition)
df_pca
ggplot(df_pca, aes(x = PC1, y = PC2, color = label)) +
  geom_point() +
  theme_minimal()
```

First 3 principal components :

```{r}
# 3D Scatter Plot
df_pca <- data.frame(pca$x[, 1:3], label = koi_disposition)
plot3d(df_pca$PC1, df_pca$PC2, df_pca$PC3, 
       col = rainbow(length(unique(df_pca$label)))[as.factor(df_pca$label)],
       size = 5, 
       xlab = "PC1", ylab = "PC2", zlab = "PC3", 
       main = "3D PCA Visualization")
```

First 5 principal components :

```{r}
# Select the first 5 principal components
df_pca_5 <- data.frame(pca$x[, 1:5], label = koi_disposition)
# Parallel coordinate plot
ggparcoord(df_pca_5, columns = 1:5, groupColumn = "label", alphaLines = 0.5)
```

Entre parenthèse KOI_table_PCA = pca$x
On ne garde que les p premières composantes :
p = 20
Pour ..
cumsum(eigenvalues)[20]/tail(cumsum(eigenvalues),1)
% de la variance totale
KOI_table_PCA = pca$x[,1:20] dim(KOI_table_PCA)

#### Outliers

```{r}
library(mice)        # Handling missing data
library(VIM)         # Visualizing missing data
```

A revoir !

```{r}
outlier_indices_list <- list()  # Initialize empty list to store indices

for (feature_name in names(KOI_table)) {
  feature <- KOI_table[[feature_name]]  # Extract column
  
  if (is.numeric(feature)) {
    # Get boxplot stats
    stats <- boxplot.stats(feature)
    
    # Identify indices of outliers
    outlier_indices <- which(feature %in% stats$out)
    
    # Store in list with feature name as key
    outlier_indices_list[[feature_name]] <- outlier_indices
    
    # Plot boxplot with labels
    boxplot(feature,
            main = paste("Boxplot of", feature_name),
            xlab = feature_name)
  }
}

```

```{r}
# Filter out empty entries (features with no outliers)
non_empty_lists <- Filter(length, outlier_indices_list)
# Get the union of all outlier indices
all_outliers <- unique(unlist(non_empty_lists))
all_outliers
dim(all_outliers)
# We lose too much information by removing the outliers
# We can do transformations 
```

```{r}
# We can't use log or box-cox transformation because we also have negative values
# However, we can use Yeo-Johnson transformation
library(e1071)
not_skewness_features <- list()
for (feature in KOI_table) {
  hist(feature)
}

#for 
#skewness(df$feature1)
```

### Classification

#### Split the data

```{r message=FALSE, warning=FALSE}
library(caret)
```

We divide the dataset into a : 
- training set 
- validation set
- testing set

```{r}
set.seed(123)  # For reproducibility

# Step 1: Split into train (70%) and temp (40%)
train_index <- createDataPartition(KOI_table$koi_disposition, p = 0.7, list = FALSE)
train_set <- KOI_table[train_index, ]
temp_set  <- KOI_table[-train_index, ]

# Step 2: Split temp into validation and test (15%)
val_index <- createDataPartition(temp_set$koi_disposition, p = 0.5, list = FALSE)
val_set <- temp_set[val_index, ]
test_set <- temp_set[-val_index, ]

train_set$koi_disposition <- as.factor(train_set$koi_disposition)
val_set$koi_disposition <- as.factor(val_set$koi_disposition)
test_set$koi_disposition <- as.factor(test_set$koi_disposition)

```

```{r}
train_index_wo <- createDataPartition(KOI_table_wo_incertitudes$koi_disposition, p = 0.7, list = FALSE)
train_set_wo <- KOI_table_wo_incertitudes[train_index, ]
temp_set_wo  <- KOI_table_wo_incertitudes[-train_index, ]

val_index_wo <- createDataPartition(temp_set$koi_disposition, p = 0.5, list = FALSE)
val_set_wo <- temp_set_wo[val_index, ]
test_set_wo <- temp_set_wo[-val_index, ]

train_set_wo$koi_disposition <- as.factor(train_set_wo$koi_disposition)
val_set_wo$koi_disposition <- as.factor(val_set_wo$koi_disposition)
test_set_wo$koi_disposition <- as.factor(test_set_wo$koi_disposition)

```

```{r}
head(train_set)
dim(train_set)
dim(val_set)
dim(test_set)

control <- trainControl(method = 'cv', number = 5)  # 5-fold CV

```

#### LDA

LDA can be robust without normality assumption
```{r warning=FALSE}
library(MASS) 
```
```{r}
class <- train_set[1]
features <- train_set[-1]

KOI_table.lda <- lda(as.matrix(features), class)
```

#### KNN

Let's try K-Nearest Neighbor method :

##### With incertitudes
```{r}
library(class) # KNN
```

```{r}
knn_model <- train(
  koi_disposition ~ ., 
  data = train_set,
  method = "knn",
  preProcess = c("center", "scale"),  # normalize predictors
  tuneLength = 10  # try different values of k (number of neighbors)
)
```

Results:
```{r}
print(knn_model)
plot(knn_model)
```
```{r}
knn_preds <- predict(knn_model, newdata = val_set)
confusionMatrix(knn_preds, val_set$koi_disposition)
```
We can see very good prediction score using KNN.
Given the high dimensional space, we still have enough data to keep the distances meaningful.

KNN doesn't have internal coefficients (it's non-parametric), so we can't extract direct weights. 
But we can use model-agnostic methods like:
```{r}
library(vip)
vip(knn_model)
```

##### Without incertitudes

```{r}
knn_model_wo <- train(
  koi_disposition ~ ., 
  data = train_set_wo,
  method = "knn",
  preProcess = c("center", "scale"),  # normalize predictors
  tuneLength = 10  # try different values of k (number of neighbors)
)
```

Results:
```{r}
print(knn_model_wo)
plot(knn_model_wo)
```
```{r}
knn_preds_wo <- predict(knn_model_wo, newdata = val_set_wo)
confusionMatrix(knn_preds_wo, val_set_wo$koi_disposition)
```


```{r}
vip(knn_model_wo)
```


#### SVM

##### With incertitudes

```{r warning=FALSE}
library(e1071)
```
```{r}
svm_model <- train(
  koi_disposition ~ .,
  data = train_set,
  method = "svmLinear",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(C = c(0.01, 0.1, 1, 10, 100))  # define multiple values of C
)
```
```{r}
print(svm_model)
plot(svm_model)  # plots accuracy vs cost (C) values

```
```{r}
svm_preds <- predict(svm_model, newdata = val_set)
confusionMatrix(svm_preds, val_set$koi_disposition)
```
We have a little better accuracy compared to KNN for the validation set: 0.9942 against 0.9856

```{r}
varImp(svm_model)
```

##### Without incertitudes

We begin by training the model on the training set
```{r}
svm_model_wo <- train(
  koi_disposition ~ .,
  data = train_set_wo,
  method = "svmLinear",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(C = c(0.01, 0.1, 1, 10, 100))  # define multiple values of C
)
```
```{r}
print(svm_model_wo)
plot(svm_model_wo)  # plots accuracy vs cost (C) values

```

We obtain an accuracy of 

```{r}
svm_preds_wo <- predict(svm_model_wo, newdata = val_set_wo)
confusionMatrix(svm_preds_wo, val_set_wo$koi_disposition)
```


```{r}
varImp(svm_model_wo)
```

#### Decision Tree

#### Random forest
```{r}
library(doParallel)
cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
rf_model <- train(koi_disposition ~ .,  data = train_set, method = "rf", trControl = control, metric = 'Accuracy',
            tuneGrid = data.frame(mtry = floor(sqrt(ncol(train_set) - 1))), ntree = 500)
print(rf_model$finalModel)

## get variable importance , and turn into a data frame
var_imp <- varImp(rf_model, scale = FALSE)$importance
var_imp <- data.frame(variable = row.names(var_imp), importance = var_imp$Overall)
ggplot(var_imp %>% arrange(importance), aes(x = reorder(variable, importance), y = importance)) + 
  geom_bar(stat = 'identity') + 
  coord_flip() +
  xlab('Variables') +
  labs(title = 'Random Forest Variable Importance') + 
  theme_minimal() +
  theme(
    axis.text = element_text(size = 10), 
    axis.title = element_text(size = 15), 
    plot.title = element_text(size = 20)
  )

## generate prediction and print the accuracy
rf_preds <- predict(rf_model, newdata = val_set)
rf_preds <- factor(rf_preds, levels = levels(val_set$koi_disposition))
accuracy <- mean(rf_preds == val_set$koi_disposition)*100
cat('Accuracy on val_set: ', round(accuracy, 2), '%', sep = '')
print(confusionMatrix(rf_preds, val_set$koi_disposition))
```
We still have a high accuracy but the precision has gone down.


#### Logistic regression

Fitting the model on the dataset

```{r}
glm.fit <- glm(koi_disposition~., data = train_set, family=binomial)
summary(glm.fit)
```
Prediction on the testing set

```{r}
# Prediction on validation set
val_probs <- predict(glm.fit, newdata = val_set, type = "response")
# Class prediction
val_pred <- ifelse(val_probs > 0.5, 1, 0)
# Confusion matrix
table(Predicted = val_pred, Actual = val_set$koi_disposition)
# Accuracy
mean(val_pred == val_set$koi_disposition)
```

#### Comparison

### Clustering

```{r}
class <- train_set[1]
features <- train_set [-1]
```

#### Hiearchical clustering
```{r}
library(cluster)
```

Compute the dissimilarity matrix
```{r}
D.e <- dist(features,method="euclidean") 
D.m <- dist(features,method="manhattan") 
D.c <- dist(features,method="canberra") 
```

Testing multiple linkage
```{r}
D.es <- hclust(D.e, method = 'single')    # closest pair of points
D.ea <- hclust(D.e, method = 'average')   # average distance between all points
D.ec <- hclust(D.e, method = 'complete')  # farthest pair of points
D.ew <- hclust(D.e, method = 'ward.D2')   # minimize increase in total within-cluster variance
```

Plot dendograms
```{r}
plot(
  D.es,
  main = 'euclidean-single',
  hang = -0.1,
  xlab = '',
  labels = F,
  cex = 0.6,
  sub = ''
)
rect.hclust(D.es, k = 2)   # 2 clusters
plot(
  D.ec,
  main = 'euclidean-complete',
  hang = -0.1,
  xlab = '',
  labels = F,
  cex = 0.6,
  sub = ''
)
rect.hclust(D.ec, k = 2)
plot(
  D.ea,
  main = 'euclidean-average',
  hang = -0.1,
  xlab = '',
  labels = F,
  cex = 0.6,
  sub = ''
)
rect.hclust(D.ea, k = 2)
```
```{r}
# Cut tree into k clusters (e.g., k = 3)
cluster_labels <- cutree(D.es, k = 2)
cluster_labels
length(which(cluster_labels==TRUE))
length(cluster_labels==FALSE)
```

```{r}
# Add cluster assignments to your data
train_set$cluster <- as.factor(cluster_labels)
table(train_set$cluster)
pca <- prcomp(features, scale. = TRUE)
plot(pca$x[,1], pca$x[,2],
     col = cluster_labels,
     pch = 19,
     xlab = "PC1", ylab = "PC2",
     main = "PCA of Features Colored by Hierarchical Clusters")
legend("topright", legend = paste("Cluster", 1:2), col = 1:2, pch = 19)
```


```{r}
library(ggplot2)

pca_df <- as.data.frame(pca$x[, 1:2])
pca_df$cluster <- as.factor(cluster_labels)

ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(size = 1.8) +
  labs(title = "PCA Colored by Clusters", color = "Cluster") +
  theme_minimal()

```


### Incertitudes influence

```{r}

```
